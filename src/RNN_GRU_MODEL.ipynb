{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-GRU-MODEL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0Mr_JR1_xsLB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNrffbNOqjiJ",
        "outputId": "7b7777cb-00ba-4019-c50a-898027173a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/projet\\ pstaln/Complex-Word_identification-main/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU-joZc8qzQD",
        "outputId": "98e39c60-fa88-4dc8-f94a-84f02e854cb1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/projet pstaln/Complex-Word_identification-main/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from util import read_data\n",
        "\n",
        "train_data_file = '../data/cwi_training/cwi_training.txt'\n",
        "test_data_file = '../data/cwi_testing_annotated/cwi_testing_annotated.txt'\n",
        "output_file = '../output/test.txt'\n",
        "\n",
        "train_sentences, train_words, train_label = read_data(train_data_file)\n",
        "test_sentences, test_words, test_label = read_data(test_data_file)"
      ],
      "metadata": {
        "id": "HcQv3cChq5se"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On concaténe les train_sentences et les test_sentences dans une seule liste"
      ],
      "metadata": {
        "id": "X8MC7fyfem7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = train_sentences + test_sentences\n",
        "\n",
        "sentences_labels = train_label + test_label"
      ],
      "metadata": {
        "id": "qDhOj9K0MEQq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_sentences) ,len(test_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YkWcZw-EiF_",
        "outputId": "8d11a9b9-c528-4747-f774-6e1415bcdf17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2237, 88221)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, on va utiliser que des phrases différents.\n",
        "\n",
        "Si on a quatres phrases avec leurs labels successives :   \n",
        "['il','joue','avec','ses',amis'] ----> [0,1,0,0,0]\n",
        "\n",
        "['il','joue','avec','ses',amis'] ----> [0,0,0,0,1]\n",
        "\n",
        "['je','vais','faire','les','courses'] ----> [0,1,0,0,0]\n",
        "\n",
        "['je','vais','faire','les','courses'] ----> [0,0,0,0,1]\n",
        "\n",
        "On va garder que deux phrases avec leurs labels :\n",
        "\n",
        "['il','joue','avec','ses',amis'] ----> [0,1,0,0,1]\n",
        "\n",
        "['je','vais','faire','les','courses'] ----> [0,1,0,0,1]"
      ],
      "metadata": {
        "id": "zkiHuWSnSzrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_sentences = [sent.split() for sent in sentences ]\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "new_label = [0 for k in list_of_sentences[0]]\n",
        "\n",
        "for i in range(0,len(list_of_sentences)-1) :\n",
        "\n",
        "  for j in range(len(list_of_sentences[i])) :\n",
        "    if sentences_labels[i][j] == 1 :\n",
        "      new_label[j] = 1\n",
        "      \n",
        "  if list_of_sentences[i] != list_of_sentences[i+1] :\n",
        "    texts.append(list_of_sentences[i])\n",
        "    labels.append(new_label)\n",
        "    new_label = [0 for k in list_of_sentences[i+1]]"
      ],
      "metadata": {
        "id": "WAIZ38LVw9SE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[10])\n",
        "print(labels[10])\n",
        "print('-------------------------------------------------------------------------------------------------------------------------')\n",
        "print(texts[150])\n",
        "print(labels[150])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2j4e6V7Ub-6",
        "outputId": "a18dd188-47dc-4777-bc47-ee9286d42717"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Pale', 'Kangaroo', 'Mouse', 'burrows', 'only', 'in', 'fine', 'sand', ',', 'while', 'the', 'Dark', 'Kangaroo', 'Mouse', 'prefers', 'fine', ',', 'gravelly', 'soils', 'but', 'may', 'also', 'burrow', 'in', 'sand', 'or', 'sandy', 'soil', '.']\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "-------------------------------------------------------------------------------------------------------------------------\n",
            "['Clicking', 'Submit', 'will', 'post', 'the', 'chosen', 'warning', 'to', 'the', 'user', \"'s\", 'talk', 'page', 'and', 'then', 'refresh', 'the', 'page', 'in', 'your', 'browser', '.']\n",
            "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0])\n",
        "labels[0]"
      ],
      "metadata": {
        "id": "08YpD5v8hrwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70a4e99-1de6-4611-8ed3-7c433110a8eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', '1832', 'his', 'family', 'emigrated', 'thence', 'to', 'Belleville', ',', 'Ontario', ',', 'where', 'he', 'apprenticed', 'with', 'the', 'printer', 'at', 'the', 'town', 'newspaper', ',', 'The', 'Belleville', 'Intelligencer', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts) , len(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_UqgsXUAA0m",
        "outputId": "0d5cf2f4-cf89-4f78-8450-c7461b63fb7b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9169, 9169)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On va mettre les labels dans un dictionnaire"
      ],
      "metadata": {
        "id": "h4UksgHSVExr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
        "label_vocab['<eos>'] = 0\n",
        "\n",
        "int_labels = []\n",
        "for label in labels:\n",
        "    int_labels.append([label_vocab[token] for token in label])\n",
        "\n",
        "print(int_labels[12])\n",
        "print(label_vocab)"
      ],
      "metadata": {
        "id": "wk9lkcNkrDb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fbd746a-7b94-4ffd-8492-c9370d174769"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "defaultdict(<function <lambda> at 0x7f1e4ae809e0>, {'<eos>': 0, 0: 1, 1: 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On va mettre les mots dans un dictionnaire pour les utiliser comme des entiers "
      ],
      "metadata": {
        "id": "WN9kpj-_VMNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = collections.defaultdict(lambda: len(vocab))\n",
        "vocab['<eos>'] = 0\n",
        "\n",
        "int_texts = []\n",
        "for text in texts:\n",
        "    int_texts.append([vocab[token.lower()] for token in text])\n",
        "\n",
        "print(int_texts[12])\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N2EcXcWyEL4",
        "outputId": "679167dc-8c43-48a9-ddcc-a91c20c0dfb4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[189, 199, 200, 201, 44, 202, 36, 203, 9, 204, 15, 205, 9, 156, 206, 9, 36, 207, 208, 209, 44, 15, 210, 21]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24028"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rev_label_vocab = {y: x for x, y in label_vocab.items()}\n",
        "rev_vocab = {y: x for x, y in vocab.items()}"
      ],
      "metadata": {
        "id": "aAGqWdivxy_L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collections.Counter ([len(h) for h in int_texts])  #dictionnaire qui compte les longueurs des phrases  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYaoOag1236K",
        "outputId": "c8a17d59-8ef4-4e0a-d299-71e78a3cf1a8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({19: 12,\n",
              "         20: 1150,\n",
              "         21: 1056,\n",
              "         22: 1038,\n",
              "         23: 979,\n",
              "         24: 911,\n",
              "         25: 847,\n",
              "         26: 710,\n",
              "         27: 654,\n",
              "         28: 615,\n",
              "         29: 543,\n",
              "         30: 498,\n",
              "         31: 29,\n",
              "         32: 15,\n",
              "         33: 16,\n",
              "         34: 20,\n",
              "         35: 18,\n",
              "         36: 15,\n",
              "         37: 10,\n",
              "         38: 8,\n",
              "         39: 18,\n",
              "         40: 7})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Puisque la plupart des phrases ont une longueur inférieur à 30 , donc on va utiliser 30 comme longueur maximale."
      ],
      "metadata": {
        "id": "vxO6ewh3Vn6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 30\n",
        "batch_size = 64 \n",
        "embed_size = 300 \n",
        "hidden_size = 128"
      ],
      "metadata": {
        "id": "2PJrUK3PyBQn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.zeros(len(int_texts), max_len).long()\n",
        "Y = torch.zeros(len(int_labels), max_len).long()\n",
        "\n",
        "for i, (text, label) in enumerate(zip(int_texts, int_labels)):\n",
        "    length = min(max_len, len(text))\n",
        "    X[i,:length] = torch.LongTensor(text[:length])\n",
        "    Y[i,:length] = torch.LongTensor(label[:length])\n",
        "\n",
        "print(X[15])\n",
        "print(Y[15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLqEDbRhyJWI",
        "outputId": "e9b4f497-4b47-4144-e91c-dd218f823332"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([233, 234, 235,   9,  15, 236, 237,  64, 238, 239, 240, 114,  61, 241,\n",
            "        242,   9, 243, 244,  23, 245, 246,   9, 247, 175,   1, 248, 249,  15,\n",
            "        250,  21])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 2, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj2N2ms0yphj",
        "outputId": "d2e87d56-3558-46b5-bd09-4dc186ce049e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([9169, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On va diviser la dataset en train_set, valid_set et test_set "
      ],
      "metadata": {
        "id": "N_BQGSW8W5j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X[:7000]\n",
        "Y_train = Y[:7000]\n",
        "X_valid = X[7000:8000] \n",
        "Y_valid = Y[7000:8000]\n",
        "X_test = X[8000:] \n",
        "Y_test = Y[8000:]"
      ],
      "metadata": {
        "id": "IMUEc9KGyOsB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On charger les X_train et X_valid dans des dataloader pour les generer en batchs dans l'entrainement"
      ],
      "metadata": {
        "id": "MMDpeGHVXRgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = TensorDataset(X_train, Y_train)\n",
        "valid_set = TensorDataset(X_valid, Y_valid)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "sskTOk_aynyn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Le modéle : "
      ],
      "metadata": {
        "id": "t-qzouf3Xf3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, label_vocab):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(len(vocab), embed_size, padding_idx=vocab['<eos>'])\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, bias=False, num_layers=1, bidirectional=False, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.decision = nn.Linear(hidden_size * 1 * 1, len(label_vocab))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        embed = self.embed(x)\n",
        "        output, hidden = self.rnn(embed)\n",
        "        output = self.decision(self.dropout(output))\n",
        "        \n",
        "        bias = torch.zeros([output.shape[0],30,3])\n",
        "\n",
        "        k1 = np.random.choice(np.arange(output.shape[0]))\n",
        "        k2 = np.random.choice(np.arange(output.shape[0]))\n",
        "        k3 = np.random.choice(np.arange(output.shape[0]))\n",
        "\n",
        "        for i in range(30):\n",
        "          bias[k1][i][2] = 4\n",
        "          bias[k2][i][2] = 4\n",
        "          bias[k3][i][2] = 4\n",
        "\n",
        "        return output + bias\n",
        "\n",
        "rnn_model = RNN( label_vocab)"
      ],
      "metadata": {
        "id": "Ulxx0h8iy0F_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perf(model, loader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    total_loss = correct = num_loss = num_perf = 0\n",
        "    for x, y in loader:\n",
        "      with torch.no_grad():\n",
        "        y_scores = model(x)\n",
        "        loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), y.view(y.size(0) * y.size(1)))\n",
        "        y_pred = torch.max(y_scores, 2)[1]\n",
        "        mask = (y != 0)\n",
        "        correct += torch.sum((y_pred.data == y) * mask)\n",
        "        total_loss += loss.item()\n",
        "        num_loss += len(y)\n",
        "        num_perf += torch.sum(mask).item()\n",
        "    return total_loss / num_loss, correct.item() / num_perf\n",
        "\n",
        "perf(rnn_model, valid_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt5HrX07zFUl",
        "outputId": "1793be7b-f1f8-4114-ff4b-63ca5612acc5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.02055590772628784, 0.2598529290258614)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, epochs, train_loader, valid_loader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(filter(lambda param: param.requires_grad, model.parameters()))\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = num = 0\n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_scores = model(x)\n",
        "            loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), y.view(y.size(0) * y.size(1)))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            num += len(y)\n",
        "        print(epoch, total_loss / num, *perf(model, valid_loader))\n",
        "\n",
        "fit(rnn_model, 10, train_loader, valid_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEAw5kK1zAWA",
        "outputId": "48b1bbe1-8376-4d24-96ec-2249a25cfea5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.003726558431982994 0.0016889898255467415 0.9693877551020408\n",
            "1 0.0016404710300266742 0.0014573358818888663 0.9721969759563744\n",
            "2 0.0014062518030405044 0.001399023100733757 0.9715359828141783\n",
            "3 0.0012175223167453493 0.0014173535630106926 0.9715359828141783\n",
            "4 0.0010521938678409372 0.0014177590385079384 0.9728992811699578\n",
            "5 0.0008806480323629719 0.0015140447616577147 0.97165991902834\n",
            "6 0.000704438866781337 0.001635498858988285 0.9733124018838305\n",
            "7 0.0005523494274488518 0.0017714847326278686 0.9701726844583988\n",
            "8 0.00044461939364139524 0.0019332203716039657 0.96909857060233\n",
            "9 0.0003546197298648102 0.002036939099431038 0.9696769396017516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On calcule l'accuracy des prédictions des mots complexes dans la test_dataset"
      ],
      "metadata": {
        "id": "gOHEtGNKX4sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.max(rnn_model(X_test), 2)[1]\n",
        "\n",
        "total_number_of_words = y_pred.shape[0] * y_pred.shape[1]\n",
        "\n",
        "train_acc = torch.sum(y_pred == Y_test)\n",
        "\n",
        "final_train_acc = train_acc/total_number_of_words\n",
        "\n",
        "print('accuracy = ',final_train_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85JqAKSAPfis",
        "outputId": "061378be-8730-4806-da11-ad83ce5e26b1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  tensor(0.9769)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vrai_positifs = 0\n",
        "faux_positifs = 0\n",
        "faux_negatifs = 0\n",
        "for i in range(y_pred.shape[0]):\n",
        "  for j in range(y_pred.shape[1]):\n",
        "    if (y_pred[i][j] == Y_test[i][j]) and (y_pred[i][j] == 2): vrai_positifs += 1\n",
        "    if (y_pred[i][j] != Y_test[i][j]) and (y_pred[i][j] == 2): faux_positifs += 1\n",
        "    if (y_pred[i][j] != Y_test[i][j]) and (y_pred[i][j] != 2): faux_negatifs += 1\n",
        "\n",
        "recall = vrai_positifs/(vrai_positifs + faux_negatifs)\n",
        "precision = vrai_positifs/(vrai_positifs + faux_positifs)\n",
        "F1_score = 2*recall*precision/(recall + precision)\n",
        "\n",
        "print('recall = ',recall)\n",
        "print('precision = ',precision)\n",
        "print('F1_score = ',F1_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAhBd34AcLqp",
        "outputId": "9878cc03-3ba4-43c8-c5b3-ac7a8c2b2c94"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recall =  0.12887438825448613\n",
            "precision =  0.22253521126760564\n",
            "F1_score =  0.16322314049586775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On va voir la prédiction des mots complexes pour quelques phrases :"
      ],
      "metadata": {
        "id": "GmRZFe8tYGR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(30,55):\n",
        "  print('the sentence is :'  ,' '.join([rev_vocab[int(X_test[k][i])] for i in range(len(X_test[k])) if rev_vocab[int(X_test[k][i])] != '<eos>']))\n",
        "\n",
        "  predicted_labels = torch.max(rnn_model(X_test[k:k+1]) , 2)[1]\n",
        "  \n",
        "  mots_complexes = []\n",
        "  for i in range(30):\n",
        "\n",
        "    if int(predicted_labels[0][i]) == 2 :\n",
        "      mots_complexes.append(rev_vocab[int(X_test[k][i])])\n",
        "      \n",
        "  print('complex words are :'  ,' , '.join(mots_complexes) ) \n",
        "  print('------------------------------------------------------------------------------------------- ') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWFKsb8vbEEC",
        "outputId": "41d5cf9e-f52a-4341-8d61-9b200985f5e8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the sentence is : may he be guard , may he be shield , for ever may he bless and wield o graciously all deeds of thine , thou dearest country mine !\n",
            "complex words are : guard , shield , graciously , deeds , thou , country , mine\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : it is found in algeria , egypt , iraq , israel , jordan , lebanon , libya , morocco , saudi arabia , syria , tunisia , and turkey .\n",
            "complex words are : \n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : its natural habitats are temperate forests , subtropical or tropical dry shrubland , mediterranean-type shrubby vegetation , and rocky areas .\n",
            "complex words are : shrubland\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : when the gaelic athletic association was founded in 1884 the english-origin name `` hurling '' was given to the men 's game .\n",
            "complex words are : english-origin , hurling , given\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : the current lord chancellor is kenneth clarke , who as with his predecessor jack straw is also secretary of state for justice .\n",
            "complex words are : jack , straw\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : the lord high chancellor of great britain , or lord chancellor is a senior and important functionary in the government of the united kingdom .\n",
            "complex words are : functionary\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : when julius caesar broke this rule , leaving his province of gaul and crossing the rubicon into italy , he precipitated a constitutional crisis .\n",
            "complex words are : province , gaul , rubicon , constitutional\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : each of these three lines was subdivided into maniples , each consisting of two centuries of 60 men commanded by the senior of the two centurions .\n",
            "complex words are : lines , maniples , centurions\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : a documentary film called chasing legends was later released that covered the 2009 tour through the eyes of htc-columbia .\n",
            "complex words are : \n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : the men will race the s-works tarmac for most road races , the s-works roubaix for cobbled classics , and the shiv tt in time trials .\n",
            "complex words are : s-works , s-works , cobbled , classics\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : dr. lothar heinrich , the team doctor since 1995 , and dr. andreas schmid set up a new comprehensive testing system .\n",
            "complex words are : \n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : these included jens heppner and christian henn in 1992 , erik zabel , rolf aldag and steffen wesemann in 1993 and jan ullrich in 1994 .\n",
            "complex words are : henn , zabel , aldag\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : as replacements , a lot of young talented riders , such as jan ghyselinck , rasmus guldhammer and martin and peter velits , were contracted .\n",
            "complex words are : \n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : the latter was team telekom 's sporting director until may 3 , 2007 when he was suspended following allegations published in former team member jef d'hont 's book .\n",
            "complex words are : allegations\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : the team achieved a great number of success , among which was steffan wesemanns win in the ronde van vlaanderen .\n",
            "complex words are : steffan , ronde\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : on june 29 , 2009 htc announced a three year sponsorship of the team beginning with the 2009 tour de france .\n",
            "complex words are : htc , announced\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : in addition the team signed many successful non-german riders such as georg totschnig , alexander vinokourov , cadel evans , santiago botero and paolo savoldelli .\n",
            "complex words are : santiago\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : jan ullrich , one of the favorites to win the race , was among those excluded from the tour .\n",
            "complex words are : \n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : he literally ran through the day , making long lists of things to be done , which he never completed .\n",
            "complex words are : \n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : this led to the irish civil war as fighting broke out in dublin between the anti-treaty ira and the provisional government 's troops .\n",
            "complex words are : \n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : worth combined individual tailoring with a standardization more characteristic of the ready-to-wear clothing industry , which was also developing during this period .\n",
            "complex words are : worth , combined , tailoring , ready-to-wear , developing\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : in order to accomplish their objective , surveyors use elements of geometry , engineering , trigonometry , mathematics , physics , and law .\n",
            "complex words are : \n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : the show will focus on hunt 's efforts to balance the demands of her professional life with her personal life .\n",
            "complex words are : \n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : in contrast , rhizomes most often have short internodes with leaf-scars and thin paper-like leaves and root along the under side of the stem .\n",
            "complex words are : leaf-scars , thin , leaves , root , stem\n",
            "------------------------------------------------------------------------------------------- \n",
            "the sentence is : runners are a type of stolon that exist above ground and are produced by many plants , such as strawberries .\n",
            "complex words are : runners , stolon , strawberries\n",
            "------------------------------------------------------------------------------------------- \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2nYB2Atq8jAt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}