{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertKeras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd6whtP7kCRV"
      },
      "outputs": [],
      "source": [
        "!pip install simpletransformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, collections\n",
        "import numpy as np\n",
        "\n",
        "def read_dataBert(path):\n",
        "    sentences = []\n",
        "    words = []\n",
        "    labels = []\n",
        "    complex_word_index = []\n",
        "    file = open(path)\n",
        "    lines = file.readlines()\n",
        "    previousSentence = None\n",
        "    currentWords = []\n",
        "    currentIndex = []\n",
        "    currentLabels= []\n",
        "    for line in lines : \n",
        "        line_split = line.split('\\t')\n",
        "        currentSentence = line_split[0]\n",
        "        if not currentSentence == previousSentence:\n",
        "          sentences.append(currentSentence)\n",
        "          previousSentence = currentSentence\n",
        "          words.append(currentWords)\n",
        "          currentWords = []\n",
        "          complex_word_index.append(currentIndex)\n",
        "          currentIndex = []\n",
        "          labels.append(currentLabels)\n",
        "          currentLabels = []\n",
        "\n",
        "        currentWords.append(line_split[1]) # word\n",
        "        cwi_index = int(line_split[2])\n",
        "        currentIndex.append(cwi_index) # word index needed for context\n",
        "        ##### pour etre entre 0 et 1\n",
        "        currentLabels.append(int(line_split[3][:-1])) # label\n",
        "        ##### pour etre entre -1 et 1\n",
        "        #currentLabels.append(int(line_split[3][:-1])*2-1) # label\n",
        "    words.pop(0); labels.pop(0); complex_word_index.pop(0)\n",
        "    return sentences, words, labels, complex_word_index\n",
        "\n",
        "def words_to_index(sentences, words):\n",
        "    vocab = collections.defaultdict(lambda: len(vocab))\n",
        "    vocab['<eos>'] = 0\n",
        "    \n",
        "    int_sentences = []\n",
        "    int_words = []\n",
        "    for i, sentence  in enumerate(sentences):\n",
        "        sentence_words = sentence.split()\n",
        "        int_sentences.append([vocab[token.lower()] for token in sentence_words])\n",
        "        int_words.append(vocab[words[i].lower()])\n",
        "            \n",
        "    return int_sentences, int_words\n",
        "\n",
        "def word_context(sentences, indecies, context_size=2):\n",
        "    pre_context, post_context = [], []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        #pre_word_context, post_word_context = [], []\n",
        "        word_index = indecies[i]\n",
        "        upper_bound = word_index + context_size + 1\n",
        "        lower_bound = word_index - context_size - 1\n",
        "        if lower_bound <= 0:\n",
        "            lower_bound = None\n",
        "        pre_context.append(sentence.split()[word_index-1 : lower_bound: -1][::-1])\n",
        "        post_context.append(sentence.split()[word_index+1 : upper_bound])\n",
        "        \n",
        "    return pre_context, post_context\n",
        "\n",
        "def tokenize(sentences):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    tokenized = []\n",
        "    for context in sentences:\n",
        "        tokens = []\n",
        "        for word in context:\n",
        "            tokenised_word = tokenizer.encode(word, add_special_tokens=True)\n",
        "            tokens += tokenised_word\n",
        "        tokenized.append(tokens)\n",
        "    return tokenized\n",
        "\n",
        "def build_attention_map(pre_tokens, word_tokens, max_size):\n",
        "    # pre_tokens, word_tokens are lists for each word ...\n",
        "    attention_maps = []\n",
        "    for i in range(len(pre_tokens)):\n",
        "        attention_map = [0]*max_size\n",
        "        max = len(pre_tokens[i])+len(word_tokens[i])\n",
        "        attention_map[len(pre_tokens[i]): max] = [1]*len(word_tokens[i])\n",
        "        attention_maps.append(attention_maps)\n",
        "            \n",
        "    return attention_maps\n",
        "\n"
      ],
      "metadata": {
        "id": "DLdsD-1Rlx1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = 'drive/MyDrive/pstaln'\n",
        "\n",
        "train_data_file = path+'/data/cwi_training/cwi_training.txt'\n",
        "test_data_file = path+'/data/cwi_testing_annotated/cwi_testing_annotated.txt'\n",
        "output_file =path+ '/output/test.txt'\n",
        "\n",
        "train_sentences, train_words, train_label, train_ind = read_dataBert(train_data_file)\n",
        "test_sentences, test_words, test_label, test_ind = read_dataBert(test_data_file)\n",
        "\n",
        "print(train_sentences[-1])\n",
        "print('words',train_words[1])\n",
        "print('labels',train_label[1])\n",
        "print('index',train_ind[1])\n",
        "#test_sentences, test_words, test_label, x2 = read_data(test_data_file)\n"
      ],
      "metadata": {
        "id": "rUIzaBR8lVdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.language_representation import RepresentationModel\n",
        "\n",
        "model = RepresentationModel(\n",
        "        model_type=\"bert\",\n",
        "        model_name=\"bert-base-uncased\",\n",
        "        use_cuda=False\n",
        "    )\n",
        "\n",
        "sentence_vectors = model.encode_sentences(train_sentences, combine_strategy=None,batch_size=len(train_sentences))"
      ],
      "metadata": {
        "id": "_HB6Nzl2kMfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_sentences), len(test_ind), len(test_label))"
      ],
      "metadata": {
        "id": "meL2YimluvLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getWordVector2(sentence_vectors, index, labels):\n",
        "  x = []\n",
        "  y = []\n",
        "  for sentence in range(len(index)):\n",
        "\n",
        "    for word in index[sentence]:\n",
        "\n",
        "      x.append(sentence_vectors[sentence][word+1])\n",
        "  for k in labels:\n",
        "    for i in k:\n",
        "      yNew = -1\n",
        "      if i == 1:\n",
        "        yNew=1 \n",
        "      y.append(yNew)\n",
        "  return x, y \n",
        "\n",
        "def getWordVector(sentence_vectors, index, labels):\n",
        "  x = []\n",
        "  y = []\n",
        "  for sentence in range(len(index)):\n",
        "\n",
        "    for word in index[sentence]:\n",
        "\n",
        "      x.append(sentence_vectors[sentence][word+1])\n",
        "  for k in labels:\n",
        "    for i in k:\n",
        "      yNew = np.zeros(2)\n",
        "      if i == 1:\n",
        "        yNew[1]=1\n",
        "      else:\n",
        "        yNew[0]=1 \n",
        "      y.append(yNew)\n",
        "  return x, y \n",
        "x, y = getWordVector(sentence_vectors, train_ind,train_label)\n",
        "print(len(x), len(y))"
      ],
      "metadata": {
        "id": "k0lcqZbkkxeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WwDTYZE2kVfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "classifier = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(2)])\n",
        "'''\n",
        "import tensorflow as tf\n",
        "classifier = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(2)])\n",
        "'''\n",
        "classifier.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "classifier.fit(np.array(x), np.array(y), epochs=10)\n"
      ],
      "metadata": {
        "id": "hrEpl6kMvKon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import tensorflow as tf\n",
        "classifier = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(2)])\n",
        "\n",
        "classifier.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "classifier.fit(np.array(x), np.array(y), epochs=10)\n",
        "'''\n",
        "def convertToArray(lst):\n",
        "  for k in range(len(lst)):\n",
        "    lst[k] = np.array(lst[k])\n",
        "  return np.array(lst)\n",
        "def split(a, n):\n",
        "    k, m = divmod(len(a), n)\n",
        "    return np.array([a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n)])\n",
        "def predict(sentences, index, classifier=classifier, embeddingModel = model):\n",
        "  a = len(sentences)\n",
        "  numberSplit = a // 100\n",
        "  sntc = split(sentences, numberSplit)\n",
        "  id = split(index, numberSplit)\n",
        "  pred = []\n",
        "  for k in range(len(sntc)):\n",
        "    print(k,' // ', numberSplit)\n",
        "    p = predictBatch(sntc[k], id[k],  classifier, embeddingModel)\n",
        "    pred.append(p)\n",
        "  return(pred)\n",
        "def predictBatch(sentences, index,classifier=classifier, embeddingModel = model):\n",
        "  sentence_vectors = model.encode_sentences(sentences, combine_strategy=None,batch_size=len(sentences))\n",
        "  x,y = getWordVector(sentence_vectors, index,[[1]])\n",
        "  x = convertToArray(x)\n",
        "  predictions = classifier.predict(x)\n",
        "  return(predictions)\n",
        "def evaluate(sentences, index,labels, classifier=classifier, embeddingModel = model):\n",
        "  a = len(sentences)\n",
        "  numberSplit = a // 100\n",
        "  sntc = split(sentences, numberSplit)\n",
        "  id = split(index, numberSplit)\n",
        "  lab = split(labels, numberSplit)\n",
        "  test_loss, test_acc = 0,0\n",
        "  for k in range(len(sntc)):\n",
        "    print(k,' // ', numberSplit)\n",
        "    l,a = evaluateBatch(sntc[k], id[k], lab[k], classifier, embeddingModel)\n",
        "    test_loss+=l; test_acc+=a\n",
        "  print('loss is : ', test_loss/numberSplit,' and acc is : ', test_acc/numberSplit)\n",
        "\n",
        "def evaluateBatch(sentences, index,labels, classifier=classifier, embeddingModel = model):\n",
        "  #print(len(sentences), len(index), len(labels))\n",
        "  sentence_vectors = model.encode_sentences(sentences, combine_strategy=None,batch_size=len(sentences))\n",
        "  x,y = getWordVector(sentence_vectors, index,labels)\n",
        "  x = convertToArray(x)\n",
        "  #print(x.shape)\n",
        "  y = convertToArray(y)\n",
        "  #print(y.shape)\n",
        "  test_loss, test_acc = classifier.evaluate(x,y, verbose=0)\n",
        "  return(test_loss, test_acc)\n",
        "\n"
      ],
      "metadata": {
        "id": "RxeivA9kwjfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Decommenter pour voir les resultats sur le test\n",
        "\n",
        "evaluate(test_sentences, test_ind, test_label)"
      ],
      "metadata": {
        "id": "52b93iuPyxib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Decommenter pour voir la recall sur le test\n",
        "\n",
        "pred = predict(test_sentences, test_ind)"
      ],
      "metadata": {
        "id": "zQPjTVnXdNoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#print(test_label)\n",
        "y = []\n",
        "for k in test_label:\n",
        "  y = y+k\n",
        "yPred = []\n",
        "for k in pred:\n",
        "  for i in k:\n",
        "    if i[0]>0.5:\n",
        "      yPred.append(1)\n",
        "    else :\n",
        "      yPred.append(0)\n",
        "\n",
        "print(len(y))\n",
        "print(len(yPred))\n",
        "'''\n"
      ],
      "metadata": {
        "id": "hrJ1YQRxT8do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pre = np.vstack(pred)\n",
        "\n",
        "def getY(labels):\n",
        "  y = []\n",
        "  for k in labels:\n",
        "    for i in k:\n",
        "      yNew = 0\n",
        "      if i == 1:\n",
        "        yNew=1\n",
        "      else:\n",
        "        yNew=0 \n",
        "      y.append(yNew)\n",
        "  return(np.array(y))\n",
        "\n",
        "def getPred(pred):\n",
        "  y = []\n",
        "  for k in pred:\n",
        "    if k[0]>k[1]:\n",
        "      y.append(0)\n",
        "    else:\n",
        "      y.append(1)\n",
        "  return(np.array(y))\n",
        "\n",
        "y = getY(test_label)\n",
        "yPre = getPred(pre)\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "rc = recall_score(y, yPred)\n",
        "print('Recall is : ', rc)"
      ],
      "metadata": {
        "id": "dccq3Eurf8VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "ps = precision_score(y, yPred)\n",
        "print('Precision score is : ', rc)"
      ],
      "metadata": {
        "id": "r7LJz5DizZiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oyfCZ2QUb_mj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}